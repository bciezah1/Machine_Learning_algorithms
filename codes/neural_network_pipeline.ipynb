{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Warning Shown\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.warn('DelftStack')\n",
    "warnings.warn('Do not show this message')\n",
    "print(\"No Warning Shown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. DefiningFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging phenotype and PRS data for a cohort\n",
    "def merging_pheno_and_prs_per_cohort(df_pheno, df_prs):\n",
    "    \"\"\"\n",
    "    Merges phenotype and PRS data based on the 'IID' column and renames key variables. \n",
    "    Filters and retains relevant columns, and removes rows with missing data (NaN).\n",
    "    \n",
    "    Args:\n",
    "        df_pheno (pd.DataFrame): The phenotype data for the cohort.\n",
    "        df_prs (pd.DataFrame): The PRS data for the cohort.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe containing merged and cleaned phenotype and PRS data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge phenotype and PRS data on 'IID' (individual ID)\n",
    "    df = df_prs.merge(df_pheno, on='IID')\n",
    "\n",
    "    # Rename 'PRS' column to 'SCORE' and 'ADRD' to 'ADRD_FINAL'\n",
    "    df = df.rename(columns={'PRS': 'SCORE', 'ADRD': 'ADRD_FINAL'})\n",
    "\n",
    "    # Select relevant columns for analysis\n",
    "    df = df[['IID', 'ADRD_FINAL', 'SCORE', 'AGE', 'SEX', 'APOEe4']]\n",
    "\n",
    "    # Remove rows with missing values (NaN)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Display the shape of the final dataframe (number of rows and columns)\n",
    "    print(df.shape)\n",
    "\n",
    "    # Return the cleaned and merged dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(cohort, path):\n",
    "    \"\"\"\n",
    "    Loads phenotype and PRS data for a given cohort, preprocesses the data by filtering \n",
    "    and standardizing key variables, and merges phenotype and PRS data into a single dataframe.\n",
    "\n",
    "    Args:\n",
    "        cohort (str): The name of the cohort (used for identifying files).\n",
    "        path (str): The path to the directory containing the cohort data files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with merged and preprocessed phenotype and PRS data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load phenotype data\n",
    "    pheno_file_path = f\"{path}{cohort}/pheno_data1.txt\"\n",
    "    df_pheno = pd.read_csv(pheno_file_path, sep='\\t')\n",
    "    #print(df_pheno)\n",
    "\n",
    "    # Load PRS data and filter based on the 'In_Regression' column\n",
    "    prs_file_path = f\"{path}{cohort}/prs_data1.txt\"\n",
    "    df_prs = pd.read_csv(prs_file_path, sep=' ')\n",
    "    df_prs = df_prs[df_prs['In_Regression'] == 'Yes']  # Keep only relevant rows for regression\n",
    "    #print(df_prs)\n",
    "\n",
    "    # Merge phenotype and PRS data into a single dataframe\n",
    "    df_ready = merging_pheno_and_prs_per_cohort(df_pheno, df_prs)\n",
    "    #print(df_ready)\n",
    "\n",
    "    # Standardize AGE and SCORE columns using z-score normalization\n",
    "    df_ready['AGE'] = zscore(df_ready['AGE'])\n",
    "    df_ready['SCORE'] = zscore(df_ready['SCORE'])\n",
    "    print(df_ready)\n",
    "\n",
    "    # Recode SEX column: 1 -> 0, 2 -> 1\n",
    "    df_ready['SEX'] = df_ready['SEX'].replace({1: 0, 2: 1})\n",
    "\n",
    "    # Return the processed dataframe\n",
    "    return df_ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute mean AUC and standard error using bootstrapping\n",
    "def bootstrap_auc(y_true, y_pred_prob, n_bootstraps=1000, random_seed=42):\n",
    "    \"\"\"\n",
    "    Computes the mean AUC and its standard error using bootstrapping.\n",
    "    \n",
    "    Args:\n",
    "        y_true (array-like): Ground truth (true binary labels).\n",
    "        y_pred_prob (array-like): Predicted probabilities for the positive class.\n",
    "        n_bootstraps (int): Number of bootstrap samples to generate. Default is 1000.\n",
    "        random_seed (int): Seed for reproducibility. Default is 42.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: mean AUC and standard error of AUC.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize random state for reproducibility\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    \n",
    "    # List to store AUC values from each bootstrap sample\n",
    "    bootstrapped_aucs = []\n",
    "    \n",
    "    # Perform bootstrapping\n",
    "    for i in range(n_bootstraps):\n",
    "        # Sample indices with replacement to create bootstrap sample\n",
    "        indices = rng.randint(0, len(y_pred_prob), len(y_pred_prob))\n",
    "        \n",
    "        # Check if the bootstrap sample contains at least one positive and one negative class\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # Skip this sample if both classes are not represented\n",
    "            continue\n",
    "        \n",
    "        # Compute the false positive rate (FPR) and true positive rate (TPR) for the bootstrap sample\n",
    "        fpr, tpr, _ = roc_curve(y_true[indices], y_pred_prob[indices])\n",
    "        \n",
    "        # Compute the AUC for the current bootstrap sample\n",
    "        score = auc(fpr, tpr)\n",
    "        \n",
    "        # Append the AUC score to the list\n",
    "        bootstrapped_aucs.append(score)\n",
    "    \n",
    "    # Convert the list of AUC scores to a numpy array\n",
    "    bootstrapped_aucs = np.array(bootstrapped_aucs)\n",
    "    \n",
    "    # Calculate the mean AUC\n",
    "    mean_auc = np.mean(bootstrapped_aucs)\n",
    "    \n",
    "    # Calculate the standard error of AUC\n",
    "    std_err_auc = np.std(bootstrapped_aucs)\n",
    "    \n",
    "    return mean_auc, std_err_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate AUC and its confidence interval\n",
    "def calculate_auc_ci(y_true, y_probs, n_bootstraps=1000, alpha=0.95):\n",
    "    \"\"\"\n",
    "    Calculates the AUC and its confidence interval using bootstrapping.\n",
    "    \n",
    "    Args:\n",
    "        y_true (array-like): Ground truth (true binary labels).\n",
    "        y_probs (array-like): Predicted probabilities for the positive class.\n",
    "        n_bootstraps (int): Number of bootstrap samples to generate. Default is 1000.\n",
    "        alpha (float): Confidence level for the interval. Default is 0.95 (95% confidence).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: mean AUC, lower bound of the confidence interval, and upper bound of the confidence interval.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to store AUC values from bootstrap samples\n",
    "    bootstrapped_aucs = []\n",
    "    \n",
    "    # Initialize random state for reproducibility\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    # Perform bootstrapping\n",
    "    for i in range(n_bootstraps):\n",
    "        # Sample indices with replacement to create bootstrap sample\n",
    "        indices = rng.randint(0, len(y_probs), len(y_probs))\n",
    "        \n",
    "        # Ensure that the bootstrap sample contains at least one positive and one negative class\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # Skip this iteration if the sample is not representative\n",
    "            continue\n",
    "        \n",
    "        # Compute the false positive rate (FPR) and true positive rate (TPR) for the bootstrap sample\n",
    "        fpr, tpr, _ = roc_curve(y_true[indices], y_probs[indices])\n",
    "        \n",
    "        # Compute the AUC for the current bootstrap sample\n",
    "        score = auc(fpr, tpr)\n",
    "        \n",
    "        # Append the AUC score to the list\n",
    "        bootstrapped_aucs.append(score)\n",
    "    \n",
    "    # Convert the list of AUC scores to a numpy array and sort the values\n",
    "    sorted_scores = np.array(bootstrapped_aucs)\n",
    "    sorted_scores.sort()\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lower_bound = np.percentile(sorted_scores, ((1.0 - alpha) / 2.0) * 100)  # Lower percentile for CI\n",
    "    upper_bound = np.percentile(sorted_scores, (alpha + ((1.0 - alpha) / 2.0)) * 100)  # Upper percentile for CI\n",
    "    \n",
    "    # Return the mean AUC, lower bound, and upper bound of the confidence interval\n",
    "    return np.mean(bootstrapped_aucs), lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a Random Forest model and plot the ROC curve\n",
    "def train_and_plot_roc_using_rf(df, cohort, base, features):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model using SMOTE for oversampling, performs hyperparameter tuning with GridSearchCV, \n",
    "    and plots the ROC curve for each feature set.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe containing the data.\n",
    "        cohort (str): The name of the cohort.\n",
    "        base (str): The base feature for comparison.\n",
    "        features (list of list): A list of feature sets to use in the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: False positive rate (FPR), true positive rate (TPR), best model, list of AUC values, \n",
    "        mean AUC values, AUC confidence intervals, and AUC sample list.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize lists to store AUC values and metrics\n",
    "    list_auc = []\n",
    "    list_mean_auc = []\n",
    "    list_std_err_auc = []\n",
    "    auc_sign_list = []\n",
    "\n",
    "    # Set up the figure for plotting ROC curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Iterate over the different feature sets\n",
    "    for feature_set, color in zip(features, ['indianred', 'cornflowerblue', 'mediumseagreen', 'dimgrey']):\n",
    "        # Select features (X) and the target variable (y)\n",
    "        X_train = df[feature_set]\n",
    "        y_train = df['ADRD_FINAL']\n",
    "\n",
    "        # Apply SMOTE to address class imbalance\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        # Split the data into training and testing sets (75% train, 25% test)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42)\n",
    "        y_train, y_test = y_train.astype('int'), y_test.astype('int')  # Ensure target is in integer format\n",
    "\n",
    "        print(\"Training data shape:\", X_train.shape)\n",
    "        print(\"Testing data shape:\", X_test.shape)\n",
    "\n",
    "        # Initialize the Random Forest model and define the hyperparameter grid\n",
    "        temp_model = RandomForestClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "            'max_depth': [None, 10, 20],     # Maximum depth of the tree\n",
    "            'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n",
    "            'min_samples_leaf': [1, 2, 4]    # Minimum number of samples required to be at a leaf node\n",
    "        }\n",
    "\n",
    "        # Perform hyperparameter tuning with GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=temp_model, param_grid=param_grid, cv=10)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Get the best model from GridSearchCV\n",
    "        model = grid_search.best_estimator_\n",
    "\n",
    "        # Get predicted probabilities for the test set\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Calculate the ROC curve and AUC\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        list_auc.append(roc_auc)\n",
    "\n",
    "        # Reset indices for y_test and y_prob to avoid mismatches\n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        y_prob = pd.Series(y_prob).reset_index(drop=True)\n",
    "\n",
    "        # Calculate mean AUC and confidence intervals using bootstrapping\n",
    "        mean_auc, std_err_auc = bootstrap_auc(y_test, y_prob)  # Mean AUC and standard error\n",
    "        mean_auc, lower_ci, upper_ci = calculate_auc_ci(y_test, y_prob)  # Mean AUC and CI\n",
    "        std_err_auc = [lower_ci, upper_ci]\n",
    "\n",
    "        # Append the calculated mean AUC and confidence interval to the lists\n",
    "        list_mean_auc.append(mean_auc)\n",
    "        list_std_err_auc.append(std_err_auc)\n",
    "\n",
    "        # Calculate confusion matrix metrics: sensitivity and specificity\n",
    "        y_pred = model.predict(X_test)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "\n",
    "        # Plot the ROC curve with AUC, sensitivity, and specificity\n",
    "        plt.plot(fpr, tpr, color=color, lw=2,\n",
    "                 label=f'Diagnosis ~ {feature_set} :\\n AUC = {roc_auc:.2f},\\n Sensitivity = {sensitivity:.2f},\\n Specificity = {specificity:.2f}')\n",
    "\n",
    "        # Track sample size and AUC value for further analysis\n",
    "        n_sample = len(y_prob)\n",
    "        print(\"AUC:\", roc_auc, \"Sample size:\", n_sample)\n",
    "        auc_sign_list.append([n_sample, roc_auc])\n",
    "\n",
    "    # Plot a diagonal line representing random guessing\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (FPR)', fontsize=20)\n",
    "    plt.ylabel('True Positive Rate (TPR)', fontsize=20)\n",
    "    plt.title(f'{base} as base, and {cohort} as target', fontsize=15)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    # Return relevant metrics and models\n",
    "    return fpr, tpr, model, list_auc, list_mean_auc, list_std_err_auc, auc_sign_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Callling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 6)\n",
      "            IID  ADRD_FINAL     SCORE       AGE  SEX  APOEe4\n",
      "0    6100022501         0.0  1.127052  1.487621  2.0     0.0\n",
      "1    2501000501         1.0  2.337579 -0.496287  2.0     0.0\n",
      "2    6100052901         1.0 -1.202366  1.983597  2.0     0.0\n",
      "3    5500000201         0.0  1.351536 -0.992264  2.0     0.0\n",
      "4    6100036101         0.0 -0.097232 -0.744275  1.0     0.0\n",
      "..          ...         ...       ...       ...  ...     ...\n",
      "395         867         0.0 -1.421055  0.619661  1.0     0.0\n",
      "396         262         0.0 -0.066769 -0.620281  2.0     0.0\n",
      "397         943         0.0 -1.452110 -1.612235  2.0     0.0\n",
      "398         189         0.0 -0.115307 -0.620281  2.0     0.0\n",
      "399         938         1.0  0.660384  1.611615  1.0     0.0\n",
      "\n",
      "[400 rows x 6 columns]\n",
      "Training data shape: (519, 1)\n",
      "Testing data shape: (173, 1)\n"
     ]
    }
   ],
   "source": [
    "# Global variables\n",
    "list_cohorts = ['data_toy']  # List of cohort datasets to be processed\n",
    "path = \"../data/\"  # Path to the folder containing the cohort data files\n",
    "base = \"Caribbean \"  # Base cohort name for labeling the plot\n",
    "\n",
    "# Iterate through the list of cohorts\n",
    "for cohort in list_cohorts:\n",
    "    # Load the data for the current cohort\n",
    "    df_ready = load_data(cohort, path)\n",
    "\n",
    "    # Initialize lists to store results for each cohort\n",
    "    list_auc_all_cohorts_rf = []  # List to store AUC values for the cohort\n",
    "    all_cohort_list_mean_auc_rf = []  # List to store mean AUC values for the cohort\n",
    "    all_cohort_list_std_auc_rf = []  # List to store AUC confidence intervals for the cohort\n",
    "    all_auc_sign_list_rf = []  # List to store AUC sample sizes and AUC values for further analysis\n",
    "\n",
    "    # Train a logistic regression model, plot the ROC curve, and collect metrics\n",
    "    fpr, tpr, model, list_auc, list_mean_auc, list_std_err_auc, auc_sign_list = train_and_plot_roc_using_rf(\n",
    "        df_ready, cohort, base, [['SCORE']]\n",
    "    )\n",
    "\n",
    "    # Append the results for this cohort to the initialized lists\n",
    "    list_auc_all_cohorts_rf.append(list_auc)  # Append the AUC values\n",
    "    all_cohort_list_mean_auc_rf.append(list_mean_auc)  # Append the mean AUC values\n",
    "    all_cohort_list_std_auc_rf.append(list_std_err_auc)  # Append the standard error/confidence interval of AUC\n",
    "    all_auc_sign_list_rf.append(auc_sign_list)  # Append sample size and AUC values\n",
    "\n",
    "    # Format the output by creating DataFrames for easier analysis\n",
    "    # Create a DataFrame to store the mean AUC for all cohorts\n",
    "    df_mean = pd.DataFrame(all_cohort_list_mean_auc_rf, columns=['mean'])\n",
    "\n",
    "    # Create a DataFrame for the confidence intervals by flattening the list and selecting CI values\n",
    "    df_ci = pd.DataFrame([item[0] for item in all_cohort_list_std_auc_rf], columns=['lower_ci', 'upper_ci'])\n",
    "\n",
    "    # Concatenate the mean AUC and confidence interval DataFrames into a single DataFrame\n",
    "    df = pd.concat([df_mean, df_ci], axis=1)\n",
    "\n",
    "    # Now, df contains the mean AUC, lower confidence interval, and upper confidence interval for the cohort\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
